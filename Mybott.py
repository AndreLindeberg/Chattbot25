import os
from dotenv import load_dotenv
import google.generativeai as genai
import streamlit as st
from pypdf import PdfReader
import re
import numpy as np
import polars as pl



# API key
load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Gemini model
model = genai.GenerativeModel("models/gemini-1.5-flash-latest")



# Function - Cosine Similarity
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) *
        np.linalg.norm(vec2))




# Function - semantic search chunks
def semantic_search_chunks(query, chunks, chunk_embeddings, k=3):
    result = genai.embed_content(
        model="models/text-embedding-004",
        content=query,
        task_type="SEMANTIC_SIMILARITY"
    )
    query_embedding = result["embedding"]

    similarity_scores = []
    for i, chunk_embedding in enumerate(chunk_embeddings):
        sim = cosine_similarity(query_embedding, chunk_embedding)
        similarity_scores.append((i, sim))

    similarity_scores.sort(key=lambda x: x[1], reverse=True)
    top_indices = [i for i, _ in similarity_scores[:k]]

    return [chunks[i] for i in top_indices]




# Function - Semantic search 
def semantic_search(query, sentences, sentence_embeddings, k=5):
    # Create embedding for question
    result = genai.embed_content(
        model="models/text-embedding-004",
        content=query,
        task_type="SEMANTIC_SIMILARITY"
    )
    query_embedding = result["embedding"]

    # Cosine Similarity between question and all sentences
    similarity_scores = []
    for i, sentence_embedding in enumerate(sentence_embeddings): 
        sim = cosine_similarity(query_embedding, sentence_embedding) 
        similarity_scores.append((i, sim)) 

    # Sort by similarity
    similarity_scores.sort(key=lambda x: x[1], reverse=True)

    # Return top sentences
    top_sentences = [i for i, _ in similarity_scores[:k]] 
    return [sentences[i] for i in top_sentences]




# Function - Semantic Chunking
def semantic_chunking(sentences, embeddings, threshold=0.8):
    chunks =[]
    current_chunk = [sentences[0]]
    current_vector = embeddings[0]

    for i in range(1, len(sentences)):
        sim = cosine_similarity(current_vector, embeddings[i])

        if sim >= threshold:
            current_chunk.append(sentences[i])
        else:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentences[i]]

        current_vector = embeddings[i]

    chunks.append(" ".join(current_chunk))
    return chunks






# Function - Embedding chunks
def embed_chunks(chunks):
    embeddings = []
    for chunk in chunks:
        try:
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=chunk,
                task_type="SEMANTIC_SIMILARITY"
            )
            embeddings.append(result["embedding"])
        except Exception as e:
            print(f"Fel i chunk: {chunk[:30]}... {e}")
            embeddings.append([0.0]*768)
    return embeddings



# Vector Store ------------------------------------------------------------------------------------------


VECTOR_STORE_PATH = "vector_store.parquet"

if os.path.exists(VECTOR_STORE_PATH):
    df = pl.read_parquet(VECTOR_STORE_PATH)
    semantic_chunks = df["chunk"].to_list()
    chunk_embeddings = df["embedding"].to_list()
    st.sidebar.success("‚úÖ Vector store laddad fr√•n disk")
else:
    st.error("vector_store.parquet saknas. Skapa den f√∂rst med prepare_data.py och ladda upp till GitHub.")
    st.stop()



# Streamlit -----------------------------------------------------------------------------------------
st.title("ü§ñ Chattbot om Typ 1-diabetes")
st.markdown(
    "<div style='color: salmon; '>‚ö†Ô∏è Den h√§r chattboten √§r ett skolprojekt och ger inte medicinska r√•d. Konsultera alltid v√•rdpersonal f√∂r fr√•gor om din h√§lsa.</div>",
    unsafe_allow_html=True
)

query = st.text_input("Vad vill du fr√•ga?")

if query:
    with st.spinner("üîç Letar efter svar i kontexten..."):

        # Search for most relevant chunks
        top_chunks = semantic_search_chunks(query, semantic_chunks, chunk_embeddings, k=2)
        context = "\n\n".join(top_chunks)

        # System prompt - limits model
        system_prompt = """Du √§r en expert p√• typ 1-diabetes.
Svara endast utifr√•n den information som ges i kontexten och ingen annan information.
Om du inte hittar svaret, skriv: 'Det vet jag inte.'
Var tydlig och anv√§nd stycken eller punktlistor om det passar."""

        # Promt for modellen
        user_prompt = f"Fr√•ga: {query}\n\nH√§r √§r kontexten:\n{context}"

        # # Generate answer from Gemini
        full_prompt = f"{system_prompt}\n\n{user_prompt}"

        response = model.generate_content(
            contents=full_prompt,
            generation_config={"temperature": 0.3, "max_output_tokens": 300}
        )


        # Show answer
        st.markdown("**üí¨ Svar fr√•n Gemini:**")
        st.write(response.text)

        # Show context
        with st.expander("üîç Visa anv√§nd kontext"):
            st.write(context)




# Redog√∂relse f√∂r hur modellen potentiellt skulle kunna  anv√§ndas i verkligheten och vilka potentiella utmaningar och m√∂jligheter som finns.

# Denna app har potential att utvecklas vidare genom att inkludera fler dokument och ut√∂kad kontext, med m√•let att st√∂tta b√•de personer som lever 
# med typ 1-diabetes och deras anh√∂riga. Det r√•der fortfarande stor okunskap om sjukdomen, vilket kan skapa oro och missf√∂rst√•nd i vardagen.
# Genom att erbjuda ett tillg√§ngligt st√∂d i form av tydliga, faktabaserade svar direkt i appen, kan anv√§ndare f√• hj√§lp i stunden n√§r fr√•gor uppst√•r.
# Appen skulle p√• sikt kunna erbjudas som ett digitalt st√∂dverktyg inom v√•rden, eller lanseras som en frist√•ende app tillg√§nglig f√∂r allm√§nheten.

# Begr√§nsningar och juridiska √∂verv√§ganden:
# Ett potentiellt problem med denna typ av chattbot √§r att svaren som ges tenderar att vara generella. 
# Eftersom varje individ reagerar olika p√• behandling och har unika behov, kan ett generiskt svar 
# aldrig ers√§tta individuell medicinsk r√•dgivning. 

# En m√∂jlig l√∂sning vore att individanpassa modellen genom att koppla den till ett personligt konto, 
# d√§r anv√§ndarens egen data (t.ex. blodsockerniv√•er, livsstil, behandling) anv√§nds f√∂r att anpassa svaren. 
# Detta skulle kr√§va omfattande resurser, s√§ker datalagring och eventuellt kontinuerlig finjustering av modellen. 
# D√§rf√∂r skulle en s√•dan l√∂sning troligen inneb√§ra att tj√§nsten blir avgiftsbelagd.

# Innan appen kan lanseras brett beh√∂ver juridiska och etiska aspekter beaktas, s√§rskilt kring ansvarsfr√•gan 
# vid felaktiga eller missf√∂rst√•dda r√•d. Det kr√§vs tydlig ansvarsfriskrivning samt eventuell samverkan med v√•rdgivare 
# f√∂r att s√§kerst√§lla att appen anv√§nds p√• ett s√§kert och korrekt s√§tt.
